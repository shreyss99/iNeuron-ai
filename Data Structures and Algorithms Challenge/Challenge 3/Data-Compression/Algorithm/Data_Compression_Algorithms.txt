(A) What is Data Compression?

Data Compression is the process of encoding information using fewer bits than a decoded representation would use through the use of specific encoding schemes.


(B) Why do we need Data Compression ?

Compression is needed because it helps to reduce the consumption of expensive resources such as a hard disk or transmission bandwidth. 
As an uncompressed text or multimedia (speech, image, or video) data requires a huge amount of bits to represent them and thus require large bandwidth.


(C) Types of Compression Techniques:

	1) Lossless:

	- In this, the redundant information contained in the data is removed.
	- Due to the removal of such information, there is no loss of the data of interest.	
	- If the data has been losslessly compressed, the original data can be recovered exactly from the compressed data.

	2) Lossy:

	- There is a loss of information in a controlled manner.
	- The lossy compression is therefore not completely reversible.
	- But the advantage of this type is higher compression ratios than the lossless compression.
	

(D) Measuring Performance of Compression Algorithms:

A way of measuring how well a compression algorithm compresses a given set of data is to look at the ratio of the number of bits required to represent the data before compression
and the number of bits required to represent the data after compression. This ratio is called the compression ratio.

Compression ration = (size of uncompressed file) / (size of compressed file)


(E) Compression Algorithms:

	1) ZIP:
	
	- ZIP is a common file format thatâ€™s used to compress one or more files together into a single location. 
	- This reduces file size and makes it easier to transport or store. 
	- A recipient can extract a ZIP file after transport and use the file in the original format. 
	- But with zipped files, the contents are compressed, which reduces the amount of data used by your computer. 
	- Another way to describe ZIP files is as an archive. The archive contains all the compressed files in one location. 
	
	2) LZW:
	
	- LZW compression is the compression of a file into a smaller file using a table-based lookup algorithm invented by Abraham Lempel, Jacob Ziv, and Terry Welch. 
	- LZW compression works by reading a sequence of symbols, grouping the symbols into strings, and converting the strings into codes.
	
	  Algorithm:
	
	  - LZW compression uses a code table, with 4096 as a common choice for the number of table entries. 
	  - Codes 0-255 in the code table are always assigned to represent single bytes from the input file.
	  - When encoding begins the code table contains only the first 256 entries, with the remainder of the table being blanks. 
	  - Compression is achieved by using codes 256 through 4095 to represent sequences of bytes.
      - As the encoding continues, LZW identifies repeated sequences in the data and adds them to the code table.
	  
	  
	   PSEUDOCODE
	   1     Initialize table with single character strings
	   2     P = first input character
	   3     WHILE not end of input stream
	   4          C = next input character
	   5          IF P + C is in the string table
	   6            P = P + C
	   7          ELSE
	   8            output the code for P
	   9          add P + C to the string table
	   10           P = C
	   11         END WHILE
	   12    output code for P 
	   
	   
	3) HUFFMAN:
	
	- It assigns variable-length codes to input characters, lengths of the assigned codes are based on the frequencies of corresponding characters. 
	- The most frequent character gets the smallest code and the least frequent character gets the largest code.
	- There are mainly two major parts in Huffman Coding:
		Build a Huffman Tree from input characters.
		Traverse the Huffman Tree and assign codes to characters.
		
		Steps to build Huffman Tree:
		- Input is an array of unique characters along with their frequency of occurrences and output is Huffman Tree. 
		- Create a leaf node for each unique character and build a min heap of all leaf nodes.
		- Extract two nodes with the minimum frequency from the min heap.
		- Create a new internal node with a frequency equal to the sum of the two nodes frequencies. 
		- Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap.
		- Repeat steps#2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete.

		Steps to print codes from Huffman Tree:
		- Traverse the tree formed starting from the root. 
		- Maintain an auxiliary array. 
		- While moving to the left child, write 0 to the array. 
		- While moving to the right child, write 1 to the array. 
		- Print the array when a leaf node is encountered.
		
		
	4) SHANNON-FANO:
	
	- Shannon Fano Algorithm is an entropy encoding technique for lossless data compression of multimedia. 
	- It assigns a code to each symbol based on their probabilities of occurrence. 
	- It is a variable-length encoding scheme.
	
		Steps:
		- Create a list of probabilities or frequency counts for the given set of symbols so that the relative frequency of occurrence of each symbol is known.
		- Sort the list of symbols in decreasing order of probability, the most probable ones to the left and least probable to the right.
		- Split the list into two parts, with the total probability of both the parts being as close to each other as possible.
		- Assign the value 0 to the left part and 1 to the right part.
		- Repeat steps 3 and 4 for each part, until all the symbols are split into individual subgroups.
		
	
	
	GENERAL OBSERVATIONS:
	
	- The ZIP Algorithm can be used to compress majority type of files of diffeent file formats - image files, PDF files, audio files, video files, text files etc.
	- The ZIP internally uses DEFLATE algorithm which works very well on text files. 
	- But the same process, when applied to other file formats does not give us good compression ratio.
	- Hence we have implemented the different copression algorithms for different file types to ensure more compression.
	- We have implemented the ZIP as the main algorithm but to improve on its efficiency we have developed other algorithms as well.	